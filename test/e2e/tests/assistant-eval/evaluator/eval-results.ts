/*---------------------------------------------------------------------------------------------
 *  Copyright (C) 2026 Posit Software, PBC. All rights reserved.
 *  Licensed under the Elastic License 2.0. See LICENSE.txt for license information.
 *--------------------------------------------------------------------------------------------*/

import { join, dirname } from 'path';
import { mkdirSync, writeFileSync, readdirSync, readFileSync, rmSync, existsSync } from 'fs';

/**
 * Model configuration: maps short names to display names and UI picker names.
 */
export const MODEL_CONFIG: Record<string, { displayName: string; pickerName: string }> = {
	'sonnet': { displayName: 'claude sonnet 4', pickerName: 'Claude Sonnet 4' },
	'opus': { displayName: 'claude opus 4', pickerName: 'Claude Opus 4' },
};

const DEFAULT_MODELS = ['sonnet'];

/**
 * Get the model keys from environment variable.
 * - EVAL_MODELS=opus â†’ ['opus']
 * - EVAL_MODELS=sonnet,opus â†’ ['sonnet', 'opus']
 * - (no env var) â†’ ['sonnet']
 */
export function getModelKeys(): string[] {
	const envModels = process.env.EVAL_MODELS?.toLowerCase();
	if (!envModels) {
		return DEFAULT_MODELS;
	}

	const models = envModels.split(',').map(m => m.trim()).filter(m => m);
	const validModels = models.filter(m => {
		if (!MODEL_CONFIG[m]) {
			console.warn(`Unknown model "${m}", skipping`);
			return false;
		}
		return true;
	});

	return validModels.length > 0 ? validModels : DEFAULT_MODELS;
}

/**
 * Get config for a specific model key.
 */
export function getModelConfig(modelKey: string): { displayName: string; pickerName: string } {
	return MODEL_CONFIG[modelKey] || MODEL_CONFIG['sonnet'];
}

/**
 * Result data from running and evaluating a test case.
 */
export interface EvalResult {
	id: string;
	description: string;
	model: string;
	response: string;
	grade: 'C' | 'P' | 'I';
	explanation: string;
	timestamp?: string;  // Auto-generated by saveResult
}

// Directory paths
const RESULTS_DIR = join(__dirname, '..', '.results');
const LOGS_DIR = join(__dirname, '..', 'logs');

/**
 * Initializes the results directory. Call in beforeAll.
 */
export function initResults(): void {
	try {
		rmSync(RESULTS_DIR, { recursive: true, force: true });
	} catch { /* ignore */ }
	mkdirSync(RESULTS_DIR, { recursive: true });
}

/**
 * Saves a single evaluation result. Call after each test.
 */
export function saveResult(result: EvalResult): void {
	const filePath = join(RESULTS_DIR, `${result.id}.json`);
	mkdirSync(dirname(filePath), { recursive: true });
	const resultWithTimestamp = { ...result, timestamp: new Date().toISOString() };
	writeFileSync(filePath, JSON.stringify(resultWithTimestamp, null, 2));
}

/**
 * Combines all results into an evaluation log and cleans up.
 * Call in afterAll. Returns the log path for attaching to test report.
 */
export function finalizeResults(): string | null {
	try {
		const resultFiles = readdirSync(RESULTS_DIR).filter(f => f.endsWith('.json'));
		const results: EvalResult[] = resultFiles.map(f =>
			JSON.parse(readFileSync(join(RESULTS_DIR, f), 'utf-8'))
		);

		if (results.length === 0) {
			return null;
		}

		mkdirSync(LOGS_DIR, { recursive: true });
		const logPath = getEvalLogPath();
		writeEvalLog(results, logPath);

		// Clean up temp directory
		rmSync(RESULTS_DIR, { recursive: true, force: true });

		if (!process.env.CI) {
			console.log(`\nðŸ“Š Evaluation log written to:\n${logPath}\n`);
		}
		return logPath;
	} catch (error) {
		console.warn('Failed to write evaluation log:\n', error);
		return null;
	}
}

/**
 * Generates a timestamp-based filename for the evaluation log.
 */
function getEvalLogPath(): string {
	const now = new Date();
	const timestamp = now.toISOString().replace(/[:.]/g, '-').slice(0, 19);
	return join(LOGS_DIR, `${timestamp}_assistant_eval.json`);
}

/**
 * Writes evaluation results to a JSON log file.
 */
function writeEvalLog(results: EvalResult[], outputPath: string): void {
	const log = {
		timestamp: new Date().toISOString(),
		summary: {
			total: results.length,
			complete: results.filter(r => r.grade === 'C').length,
			partial: results.filter(r => r.grade === 'P').length,
			incomplete: results.filter(r => r.grade === 'I').length,
			passRate: results.filter(r => r.grade !== 'I').length / results.length,
		},
		results: results.map(r => ({
			id: r.id,
			description: r.description,
			model: r.model,
			grade: r.grade,
			explanation: r.explanation,
			timestamp: r.timestamp,
		})),
	};

	writeFileSync(outputPath, JSON.stringify(log, null, 2));
}

/**
 * Minimal test case info needed for catalog generation.
 */
interface CatalogTestCase {
	id: string;
	description: string;
	prompt: string;
	mode: 'Ask' | 'Edit' | 'Agent';
	tags?: { toString(): string }[];
	evaluationCriteria: {
		essential: string[];
		additional?: string[];
		failIf?: string[];
	};
}

/**
 * Generates a markdown catalog of all test cases.
 * Call in afterAll to keep the catalog up to date.
 * Skipped in CI environments.
 */
export function generateCatalog(testCases: CatalogTestCase[]): void {
	if (process.env.CI) {
		return;
	}

	const catalogPath = join(__dirname, '..', 'LLM_EVAL_TEST_CATALOG.md');

	// Sort by ID for deterministic order
	const sortedCases = [...testCases].sort((a, b) => a.id.localeCompare(b.id));

	generateCatalogMarkdown(sortedCases, catalogPath);
}

/**
 * Generates a GitHub-compatible markdown catalog of all test cases.
 */
function generateCatalogMarkdown(testCases: CatalogTestCase[], outputPath: string): void {
	const timestamp = new Date().toISOString();
	const lines: string[] = [];

	// Header
	lines.push('# Positron: LLM Eval Test Catalog');
	lines.push('');
	lines.push(`> ${testCases.length} test cases Â· Auto-generated on ${timestamp}`);
	lines.push('');

	// Summary table
	lines.push('## Summary');
	lines.push('');
	lines.push('| ID | Description | Mode | Tags |');
	lines.push('|----|-------------|------|------|');
	for (const tc of testCases) {
		const tags = tc.tags?.map(t => `\`${String(t)}\``).join(' ') || 'â€”';
		lines.push(`| [${tc.id}](#${tc.id.toLowerCase().replace(/[^a-z0-9]/g, '-')}) | ${tc.description} | ${tc.mode} | ${tags} |`);
	}
	lines.push('');

	// Test case details (collapsible)
	lines.push('## Test Cases');
	lines.push('');

	for (const tc of testCases) {
		const tags = tc.tags?.map(t => `\`${String(t)}\``).join(' ') || '';

		// Collapsible section with summary
		lines.push(`<details>`);
		lines.push(`<summary><strong>${tc.id}</strong> â€” ${tc.description}</summary>`);
		lines.push('');
		lines.push(`**Mode:** ${tc.mode}${tags ? ` | **Tags:** ${tags}` : ''}`);
		lines.push('');

		// Prompt
		lines.push('#### Prompt');
		lines.push('');
		lines.push('```');
		lines.push(tc.prompt);
		lines.push('```');
		lines.push('');

		// Criteria
		lines.push('#### Criteria');
		lines.push('');

		// Essential
		lines.push('**Essential**');
		lines.push('');
		for (const c of tc.evaluationCriteria.essential) {
			lines.push(`- âœ“ ${c}`);
		}
		lines.push('');

		// Additional
		if (tc.evaluationCriteria.additional?.length) {
			lines.push('**Additional**');
			lines.push('');
			for (const c of tc.evaluationCriteria.additional) {
				lines.push(`- Â· ${c}`);
			}
			lines.push('');
		}

		// Fail if
		if (tc.evaluationCriteria.failIf?.length) {
			lines.push('**Fail if**');
			lines.push('');
			for (const c of tc.evaluationCriteria.failIf) {
				lines.push(`- âœ— ${c}`);
			}
			lines.push('');
		}

		lines.push('</details>');
		lines.push('');
	}

	const markdown = lines.join('\n');

	// Check if content has changed (ignore timestamp differences)
	const stripTimestamp = (content: string) =>
		content.replace(/Auto-generated on \d{4}-\d{2}-\d{2}T[\d:.]+Z/g, 'Auto-generated on [TIMESTAMP]');

	let shouldWrite = true;
	if (existsSync(outputPath)) {
		const existing = readFileSync(outputPath, 'utf-8');
		if (stripTimestamp(existing) === stripTimestamp(markdown)) {
			shouldWrite = false;
		}
	}

	if (shouldWrite) {
		writeFileSync(outputPath, markdown);
		console.log(`ðŸ“„ Markdown catalog updated:\n${outputPath}\n`);
	} else {
		console.log(`ðŸ“„ Markdown catalog unchanged, skipping write.\n`);
	}
}
