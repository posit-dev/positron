/*---------------------------------------------------------------------------------------------
 *  Copyright (C) 2026 Posit Software, PBC. All rights reserved.
 *  Licensed under the Elastic License 2.0. See LICENSE.txt for license information.
 *--------------------------------------------------------------------------------------------*/

import { join, dirname } from 'path';
import { mkdirSync, writeFileSync, readdirSync, readFileSync, rmSync, existsSync } from 'fs';

/**
 * Model configuration: maps short names to display names and UI picker names.
 */
export const MODEL_CONFIG: Record<string, { displayName: string; pickerName: string }> = {
	'sonnet': { displayName: 'claude sonnet 4', pickerName: 'Claude Sonnet 4' },
	'opus': { displayName: 'claude opus 4', pickerName: 'Claude Opus 4' },
};

const DEFAULT_MODELS = ['sonnet'];

/**
 * Get the model keys from environment variable.
 * - EVAL_MODELS=opus â†’ ['opus']
 * - EVAL_MODELS=sonnet,opus â†’ ['sonnet', 'opus']
 * - (no env var) â†’ ['sonnet']
 */
export function getModelKeys(): string[] {
	const envModels = process.env.EVAL_MODELS?.toLowerCase();
	if (!envModels) {
		return DEFAULT_MODELS;
	}

	const models = envModels.split(',').map(m => m.trim()).filter(m => m);
	const validModels = models.filter(m => {
		if (!MODEL_CONFIG[m]) {
			console.warn(`Unknown model "${m}", skipping`);
			return false;
		}
		return true;
	});

	return validModels.length > 0 ? validModels : DEFAULT_MODELS;
}

/**
 * Get config for a specific model key.
 */
export function getModelConfig(modelKey: string): { displayName: string; pickerName: string } {
	return MODEL_CONFIG[modelKey] || MODEL_CONFIG['sonnet'];
}

/**
 * Result data from running and evaluating a test case.
 */
export interface EvalResult {
	id: string;
	description: string;
	model: string;
	response: string;
	grade: 'C' | 'P' | 'I';
	explanation: string;
	timestamp?: string;  // Auto-generated by saveResult
}

// Directory paths
const RESULTS_DIR = join(__dirname, '..', '.results');
const LOGS_DIR = join(__dirname, '..', 'logs');

/**
 * Initializes the results directory. Call in beforeAll.
 */
export function initResults(): void {
	try {
		rmSync(RESULTS_DIR, { recursive: true, force: true });
	} catch { /* ignore */ }
	mkdirSync(RESULTS_DIR, { recursive: true });
}

/**
 * Saves a single evaluation result. Call after each test.
 */
export function saveResult(result: EvalResult): void {
	const filePath = join(RESULTS_DIR, `${result.id}.json`);
	mkdirSync(dirname(filePath), { recursive: true });
	const resultWithTimestamp = { ...result, timestamp: new Date().toISOString() };
	writeFileSync(filePath, JSON.stringify(resultWithTimestamp, null, 2));
}

/**
 * Combines all results into an evaluation log and cleans up.
 * Call in afterAll. Returns the log path for attaching to test report.
 */
export function finalizeResults(): string | null {
	try {
		const resultFiles = readdirSync(RESULTS_DIR).filter(f => f.endsWith('.json'));
		const results: EvalResult[] = resultFiles.map(f =>
			JSON.parse(readFileSync(join(RESULTS_DIR, f), 'utf-8'))
		);

		if (results.length === 0) {
			return null;
		}

		mkdirSync(LOGS_DIR, { recursive: true });
		const logPath = getEvalLogPath();
		writeEvalLog(results, logPath);

		// Clean up temp directory
		rmSync(RESULTS_DIR, { recursive: true, force: true });

		console.log(`\nðŸ“Š Evaluation log written to:\n${logPath}\n`);
		return logPath;
	} catch (error) {
		console.warn('Failed to write evaluation log:\n', error);
		return null;
	}
}

/**
 * Generates a timestamp-based filename for the evaluation log.
 */
function getEvalLogPath(): string {
	const now = new Date();
	const timestamp = now.toISOString().replace(/[:.]/g, '-').slice(0, 19);
	return join(LOGS_DIR, `${timestamp}_assistant_eval.json`);
}

/**
 * Writes evaluation results to a JSON log file.
 */
function writeEvalLog(results: EvalResult[], outputPath: string): void {
	const log = {
		timestamp: new Date().toISOString(),
		summary: {
			total: results.length,
			complete: results.filter(r => r.grade === 'C').length,
			partial: results.filter(r => r.grade === 'P').length,
			incomplete: results.filter(r => r.grade === 'I').length,
			passRate: results.filter(r => r.grade !== 'I').length / results.length,
		},
		results: results.map(r => ({
			id: r.id,
			description: r.description,
			model: r.model,
			grade: r.grade,
			explanation: r.explanation,
			timestamp: r.timestamp,
		})),
	};

	writeFileSync(outputPath, JSON.stringify(log, null, 2));
}

/**
 * Minimal test case info needed for catalog generation.
 */
interface CatalogTestCase {
	id: string;
	description: string;
	prompt: string;
	mode: 'Ask' | 'Edit' | 'Agent';
	tags?: { toString(): string }[];
	evaluationCriteria: {
		essential: string[];
		additional?: string[];
		failIf?: string[];
	};
}

/**
 * Generates a markdown catalog of all test cases.
 * Call in afterAll to keep the catalog up to date.
 * Only writes if content has changed (ignores timestamp line).
 * Skipped in CI environments.
 */
export function generateCatalog(testCases: CatalogTestCase[]): void {
	if (process.env.CI) {
		return;
	}

	const catalogPath = join(__dirname, '..', 'EVAL_CATALOG.md');

	// Sort by ID for deterministic order
	const sortedCases = [...testCases].sort((a, b) => a.id.localeCompare(b.id));

	// Build content lines (without timestamp - we'll add it conditionally)
	const contentLines: string[] = [];

	// Summary table
	contentLines.push('## Summary');
	contentLines.push('');
	contentLines.push('| ID | Description | Prompt | Mode | Tags |');
	contentLines.push('|----|-------------|--------|------|------|');
	for (const tc of sortedCases) {
		const tags = tc.tags?.map(t => `\`${t}\``).join(', ') || '';
		contentLines.push(`| ${tc.id} | ${tc.description} | ${tc.prompt} | ${tc.mode} | ${tags} |`);
	}

	contentLines.push('');
	contentLines.push('---');
	contentLines.push('');

	// Detailed collapsible sections
	for (const tc of sortedCases) {
		contentLines.push(`### ${tc.id}`);
		contentLines.push('');
		contentLines.push('<details>');
		contentLines.push(`<summary><strong>${tc.description}</strong></summary>`);
		contentLines.push('');
		contentLines.push('**Prompt**');
		contentLines.push('');
		contentLines.push('```');
		contentLines.push(tc.prompt);
		contentLines.push('```');
		contentLines.push('');
		contentLines.push('#### Grading criteria');
		contentLines.push('');
		contentLines.push('**Essential**');
		for (const c of tc.evaluationCriteria.essential) {
			contentLines.push(`- ${c}`);
		}
		contentLines.push('');
		if (tc.evaluationCriteria.additional?.length) {
			contentLines.push('**Additional**');
			for (const c of tc.evaluationCriteria.additional) {
				contentLines.push(`- ${c}`);
			}
			contentLines.push('');
		}
		if (tc.evaluationCriteria.failIf?.length) {
			contentLines.push('**Fail if**');
			for (const c of tc.evaluationCriteria.failIf) {
				contentLines.push(`- ${c}`);
			}
			contentLines.push('');
		}
		contentLines.push('</details>');
		contentLines.push('');
	}

	const newContent = contentLines.join('\n');

	// Check if content has changed (strip header/timestamp from existing file)
	let shouldWrite = true;
	if (existsSync(catalogPath)) {
		const existing = readFileSync(catalogPath, 'utf-8');
		// Header is 4 lines: title, blank, timestamp, blank (indices 0-3)
		// Content starts at index 4 with "## Summary"
		const existingContent = existing.split('\n').slice(4).join('\n');
		if (existingContent.trim() === newContent.trim()) {
			shouldWrite = false;
		}
	}

	if (shouldWrite) {
		// 5 elements joined by \n produces: title\n\ntimestamp\n\n (4 newlines total)
		// This creates 4 header lines, then content starts at line 5 (index 4)
		const header = [
			'# LLM Eval Test Catalog',
			'',
			`> Auto-generated on ${new Date().toISOString()} â€” do not edit`,
			'',
			'',
		].join('\n');
		writeFileSync(catalogPath, header + newContent);
		console.log(`ðŸ“š Test catalog updated:\n${catalogPath}\n`);
	} else {
		console.log(`ðŸ“š Test catalog unchanged, skipping write.\n`);
	}
}
