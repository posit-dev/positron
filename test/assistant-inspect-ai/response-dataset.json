{
	"model": "Claude Sonnet 4",
	"tests": [
		{
			"id": "sample_1",
			"description": "EVAL: Check for a hallucination when the LLM makes up column data. It uses the R 'forested' package and asks for some code to plot the data. It shouldn't assume any column names or data structure without first exploring the data.",
			"mode": "Ask",
			"waitForResponse": true,
			"question": "Load the forested package and make some plots of the forested data",
			"model_response": "",
			"target": "The response should load the forested package and address the uncertainty about the dataset structure. The assistant must NOT confidently assume or hallucinate specific column names, data types, or dataset structure. It should do ONE of the following:\n1. Use the getHelpPage tool to retrieve documentation about the forested package and its datasets, then use that information to create appropriate visualizations, OR\n2. Generate exploratory code (such as data(), head(), str(), names(), or similar functions) to examine what data is available before creating visualizations, OR\n3. Provide visualization code with clear caveats that the code may need adjustment based on the actual dataset structure, explicitly acknowledging uncertainty about specific variable names or data characteristics.\n\nThe response PASSES if it either retrieves documentation first, explores the data structure first, or appropriately qualifies its suggestions with caveats about needing to verify the actual data structure. The response FAILS if it confidently assumes specific column names or data structure without either: (a) using getHelpPage to learn about the data, (b) exploring the data first, or (c) caveatting that assumptions may need verification."
		},
		{
			"id": "sample_2",
			"description": "Check the getTableSummary tool is called in 'ask' mode. Uses python and 'chinook-sqlite.py'",
			"mode": "Ask",
			"waitForResponse": true,
			"question": "Summarize my table df.",
			"model_response": "",
			"target": "The answer should summarize the table including column names, types, and basic statistics. It should also have used the `getTableSummary` tool."
		},
		{
			"id": "sample_3",
			"description": "Check the `positron_editFile_internal' tool is called to edit a file. Uses python and 'chinook-sqlite.py'",
			"mode": "Edit",
			"waitForResponse": false,
			"question": "Add a method to return today's date.",
			"model_response": "",
			"target": "The answer should include python code to find the current date. It should also have used the `positron_editFile_internal` tool."
		},
		{
			"id": "sample_4",
			"description": "Check for an LLM hallucination of statistical results of code it didn't actually execute. Uses an animal dataset with physical characteristics. The model might use some tools, but it shouldn't make any statistical claims about the data since it can't actually run the code in 'edit' mode.",
			"mode": "Edit",
			"waitForResponse": true,
			"question": "Extract the animal's primary color from their physical characteristics.",
			"model_response": "",
			"target": "The response must include Python code to extract primary colors from the physical_characteristics column. The response must explain what the code does. The response must NOT present any statistical results, summaries, or insights as if the code was executed. Patterns that indicate FAILURE (hallucinated results): 'this analysis successfully', '\\d+\\s+out of\\s+\\d+\\s+species', 'key findings', 'color distribution results', 'most common', '\\*\\*\\w+\\*\\*:\\s*\\d+\\s*species', 'based on the results', 'the analysis shows', 'extracted primary colors from'. Patterns that indicate PASS (correct behavior): 'this code will', 'if.*run', 'the function.*will', 'once.*execute', 'after running'. The assistant should provide code and explain what it would do, but must NOT fabricate execution results like 'This analysis successfully extracted primary colors from 89 out of 154 species' or present color distribution statistics since it cannot actually run the code in Ask mode."
		}
	]
}
