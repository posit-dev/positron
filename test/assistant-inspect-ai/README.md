# Positron LLM Inspect AI Testing

This documents the process for evaluating the performance of the Positron Assistant.

This testing process consists of 2 parts:
1. A Playwright e2e test that runs the assistant in a controlled environment and captures the responses.
2. An Inspect AI evaluation that scores the responses based on a predefined rubric.

## Files Overview

### `response-dataset.json`
This file contains the dataset of questions and evaluation criteria. Structure:

- `models`: Array of model names to test (e.g., `["Claude Sonnet 4", "Claude Opus 4.5"]`)
- `tests`: Array of test cases, each containing:
  - `id`: A unique identifier for the test case (e.g., `sample_1`)
  - `description`: A brief description of the test case
  - `mode`: The mode of interaction (`Ask`, `Edit`, or `Agent`)
  - `waitForResponse`: Whether to wait for the assistant to finish responding
  - `question`: The question or prompt given to the assistant
  - `model_response`: Populated during the e2e test (leave blank)
  - `target`: Evaluation criteria using ESSENTIAL/ADDITIONAL format for grading

### `actions/` Directory
Contains per-sample setup and cleanup actions for test cases:

- `types.ts`: Defines `ActionContext` and `SampleActions` interfaces
- `index.ts`: Exports the `getActionsForSample()` function
- `sample_N.ts`: Individual action files for each sample that needs setup/cleanup

Each sample action file can export:
- `setup`: Runs before the question is asked (e.g., open files, start sessions)
- `postQuestion`: Runs after the question but before capturing the response
- `cleanup`: Runs after the response is captured (e.g., close editors, discard changes)

### Output Files
The e2e test generates per-model output files: `response-dataset-{model-name}.json`
(e.g., `response-dataset-claude-sonnet-4.json`)

Each output file contains a `model` field and the `tests` array with populated `model_response` fields.

## Playwright E2E Test

Located in `test/e2e/tests/inspect-ai/inspect-ai.test.ts`, this test:
1. Loads the dataset and iterates through each model in the `models` array
2. For each model, processes all questions and captures responses
3. Writes separate output files for each model

Requires the `ANTHROPIC_KEY` environment variable when using `USE_KEY=true`. If the environment
variable `ANTHROPIC_API_KEY` is set, then sign-in happens automatically and `USE_KEY=false` should be used.

## Inspect AI Evaluation

Located in `json-response-eval.py`, this script evaluates the responses using the Inspect AI framework.

- Uses Claude Haiku 4.5 as the scorer model
- Grades responses as Complete (C), Partial (P), or Incomplete (I)
- Supports custom input files via the `INPUT_FILENAME` environment variable
- Requires an `ANTHROPIC_API_KEY` environment variable for the scorer model

### Result Parser

`inspect_result_parser.py` parses evaluation results and determines pass/fail based on an accuracy threshold:

```bash
python inspect_result_parser.py <json_log_file> --threshold 0.8
```

## Running the Evaluation

### 1. Run the e2e test to capture responses

```bash
npx playwright test --project e2e-electron
```

### 2. Run the Inspect AI evaluation for each model

```bash
cd test/assistant-inspect-ai
pip install -r requirements.txt

# Evaluate a specific model's responses
INPUT_FILENAME=response-dataset-claude-sonnet-4.json inspect eval ./json-response-eval.py

# Or evaluate the default file
inspect eval ./json-response-eval.py
```

### 3. Parse results (optional)

```bash
python inspect_result_parser.py ./logs/<log_file>.json --threshold 0.8
```

This will generate a report in the `./logs` directory.

It's recommended to install the [Inspect AI VSCode extension](https://marketplace.visualstudio.com/items?itemName=ukaisi.inspect-ai) for easier viewing of the evaluation results.

## GitHub Actions Workflow

The workflow (`.github/workflows/test-assistant-llm.yml`) runs the full evaluation process automatically.

### Schedule
- **Nightly at 2am UTC (10pm EST)** on weekdays (Monday-Friday)
- Can also be triggered manually via `workflow_dispatch`

### Workflow Steps

1. **e2e-electron**: Runs the Playwright e2e test on Ubuntu to capture responses for all models in the dataset
2. **inspect-ai-test**: Downloads the response artifacts and runs Inspect AI evaluation for each model
3. **slack-notify**: Sends notification to `#positron-test-results` Slack channel on failure

### Accuracy Threshold
Each model must achieve **80% accuracy** to pass. If any model fails to meet the threshold, the workflow fails and triggers a Slack notification.

### Artifacts
- `inspect-ai-responses`: Response dataset files generated by the e2e test
- `inspect-ai-logs`: Inspect AI evaluation logs (uploaded regardless of pass/fail)
