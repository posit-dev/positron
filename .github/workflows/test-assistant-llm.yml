name: "Test: Assistant LLM Testing"

# Run builds daily at 2am UTC (10p EST) on weekdays for now, or manually
on:
  schedule:
    - cron: "0 2 * * 1-5"
  workflow_dispatch:
    inputs:
      notify_on:
        description: "Slack notification on:"
        required: false
        default: "failure"
        type: choice
        options:
          - failure
          - always
          - never

jobs:  # to input into inspect-ai-test job below

  e2e-electron:
    name: e2e
    uses: ./.github/workflows/test-e2e-ubuntu.yml
    with:
      grep: ""
      project: "inspect-ai"
      display_name: "electron (ubuntu)"
      currents_tags: "merge,electron/ubuntu"
      install_undetectable_interpreters: false
      install_license: false
      upload_logs: false
      report_currents: false
      skip_extension_test: true
      workers: 1
    secrets: inherit

  # Windows works, but takes a long time and need to add processing of multiple artifacts
  # e2e-windows-electron:
    # name: e2e
    # uses: ./.github/workflows/test-e2e-windows-run.yml
    # with:
    #   grep: ""
    #   project: "inpsect-ai"
    #   display_name: "electron (windows)"
    #   currents_tags: "merge,electron/windows"
    #   upload_logs: false
    #   report_currents: false
    #   skip_extension_test: true
    # secrets: inherit

  inspect-ai-test:
    needs: [e2e-electron]
    name: inspect-ai-test
    timeout-minutes: 120
    runs-on: ubuntu-latest

    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
          submodules: recursive

      - name: Load secret
        uses: 1password/load-secrets-action@v3
        with:
          # Export loaded secrets as environment variables
          export-env: true
        env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}
          ANTHROPIC_API_KEY: "op://Positron/Anthropic/credential"

      - name: Download artifacts
        uses: actions/download-artifact@v7
        with:
          name: inspect-ai-responses
          path: /home/runner/work/positron/positron/test/assistant-inspect-ai

      - name: Setup Python environment
        run: |
          cd test/assistant-inspect-ai
          python3 -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Run Inspect AI JSON Evaluation for all models
        run: |
          cd test/assistant-inspect-ai
          source venv/bin/activate

          # Find all response dataset files (excluding the template file)
          RESPONSE_FILES=$(ls response-dataset-*.json 2>/dev/null || true)

          if [ -z "$RESPONSE_FILES" ]; then
            echo "No response-dataset-*.json files found!"
            exit 1
          fi

          echo "Found response files: $RESPONSE_FILES"

          # Run inspect eval for each model's response file
          for response_file in $RESPONSE_FILES; do
            echo ""
            echo "=========================================="
            echo "Running evaluation for: $response_file"
            echo "=========================================="
            INPUT_FILENAME="$response_file" inspect eval json-response-eval.py --log-format json --log-dir logs/
          done

      - name: Parse results and check threshold for all models
        run: |
          cd test/assistant-inspect-ai

          # Count number of response files to know how many logs to process
          NUM_MODELS=$(ls response-dataset-*.json 2>/dev/null | wc -l)
          echo "Processing results for $NUM_MODELS model(s)"

          # Find the most recent log files (one per model), handling spaces in filenames
          # Use find with -printf to get modification time, sort, and take the most recent
          FAILED=0
          PROCESSED=0

          while IFS= read -r -d '' log_file; do
            echo ""
            echo "=========================================="
            echo "Parsing results from: $log_file"
            echo "=========================================="

            # Parse results with 80% accuracy threshold
            # Don't exit immediately on failure, track failures instead
            if ! python inspect_result_parser.py "$log_file" --threshold 0.8; then
              FAILED=$((FAILED + 1))
            fi
            PROCESSED=$((PROCESSED + 1))
          done < <(find logs -maxdepth 1 -name "*.json" -printf '%T@\t%p\0' | sort -rzn | head -zn "$NUM_MODELS" | cut -zf2-)

          if [ $PROCESSED -eq 0 ]; then
            echo "No log files found!"
            exit 1
          fi

          echo ""
          echo "=========================================="
          echo "OVERALL SUMMARY"
          echo "=========================================="
          if [ $FAILED -gt 0 ]; then
            echo "FAIL: $FAILED model(s) did not meet the accuracy threshold"
            exit 1
          else
            echo "PASS: All $NUM_MODELS model(s) met the accuracy threshold"
          fi

      - name: Upload inspect-ai eval logs
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: inspect-ai-logs
          path: test/assistant-inspect-ai/logs/*

  slack-notify:
    if: failure()
    needs:
      [
        inspect-ai-test,
        e2e-electron,
      ]
    runs-on: ubuntu-latest
    env:
      notify_on: ${{ github.event_name == 'schedule' && 'always' || inputs.notify_on || 'failure' }}
    steps:
      - name: Notify Slack
        uses: midleman/slack-workflow-status@v3.1.3
        with:
          gh_repo_token: ${{ secrets.GITHUB_TOKEN }}
          slack_token: ${{ secrets.SLACK_TOKEN_TEST_STATUS }}
          slack_channel: "#positron-test-results"
          notify_on: ${{ env.notify_on }}
          include_commit_msg: false
          include_job_statuses: "on-failure"
          include_job_durations: false
          custom_title: "Nightly Inspect AI LLM Evaluation"

